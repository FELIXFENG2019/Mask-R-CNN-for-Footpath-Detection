{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"footpath_rgbd.ipynb","provenance":[],"collapsed_sections":["mMbIVrF_4oQc"],"authorship_tag":"ABX9TyM1kJtZauH9th5lvKTSbB+V"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mMbIVrF_4oQc","colab_type":"text"},"source":["## Configuration Script"]},{"cell_type":"code","metadata":{"id":"sSeqxHLr4UGe","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QmhtL334rxi","colab_type":"code","colab":{}},"source":["# 安装最新版本Keras\n","# https://keras.io/\n","#!pip install keras\n","#最新版 keras==2.3.0 2020.06.27\n","# 指定版本安装\n","#!pip install keras==2.0.8\n","!pip install keras==2.1.5\n","\n","%tensorflow_version 1.x\n","!pip install imgaug==0.2.5\n","!pip install gast==0.2.2\n","\n","# 安装 OpenCV\n","# https://opencv.org/\n","!apt-get -qq install -y libsm6 libxext6 && pip install -q -U opencv-python\n","# 安装 Pytorch\n","# http://pytorch.org/\n","!pip install --upgrade pip\n","!pip install folium==0.2.1\n","!pip install numpy --upgrade\n","!pip install torch==1.5.1\n","# 安装 XGBoost\n","# https://github.com/dmlc/xgboost\n","!pip install -q xgboost\n","# 安装 7Zip\n","!apt-get -qq install -y libarchive-dev && pip install -q -U libarchive\n","# 安装 GraphViz 和 PyDot\n","!apt-get -qq install -y graphviz && pip install -q pydot\n","\n","#import tensorflow\n","#print(tensorflow.__version__)\n","\n","!pip install --upgrade pip\n","!pip install folium==0.2.1\n","!pip install numpy --upgrade\n","!pip install torch==1.5.1\n","\n","import os\n","#from google.colab import drive\n","#drive.mount('/content/drive',force_remount=True)\n"," \n","path = \"/content/drive/My Drive/Colab Notebooks/Mask_RCNN-master\"\n","os.chdir(path)\n","\n","!pip install -r requirements.txt\n","\n","!python setup.py install\n","\n","#path = \"/content/drive/My Drive/Colab Notebooks/coco-master/PythonAPI\"\n","#os.chdir(path)\n","\n","#!make"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaSSEJKdPe-Y","colab_type":"text"},"source":["## Mask R-CNN"]},{"cell_type":"code","metadata":{"id":"4v0oNm1Z4yWK","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import random\n","import math\n","import re\n","import time\n","import numpy as np\n","import cv2\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import json\n","import datetime\n","import skimage.draw\n","import imgaug.augmenters as iaa\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"../../\") \n","#ROOT_DIR = os.path.abspath(\"/content/drive/My Drive/Colab Notebooks/Mask_RCNN-master\")\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR)  # To find local version of the library\n","from mrcnn.config import Config\n","from mrcnn import utils\n","import mrcnn.model as modellib\n","from mrcnn import visualize\n","from mrcnn.model import log\n","\n","%matplotlib inline \n","\n","# Directory to dataset\n","dataset_dir = os.path.abspath(\"/content/drive/My Drive/Colab Notebooks/Mask_RCNN-master/samples/footpath/footpath dataset 2.0\")\n","\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n","\n","# Local path to pre-trained weights file (transfer learning)\n","DIODE_DEPTH_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_diode_depth.h5\")\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n","# Download COCO trained weights from Releases if needed\n","if not os.path.exists(COCO_MODEL_PATH):\n","    utils.download_trained_weights(COCO_MODEL_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbgaY4ZqQAfD","colab_type":"text"},"source":["## Configurations"]},{"cell_type":"code","metadata":{"id":"_K5QOQjQP8FQ","colab_type":"code","colab":{}},"source":["# Configurations\n","class FootpathConfig(Config):\n","    \"\"\"Configuration for training on the footpath dataset.\n","    Derives from the base Config class and overrides values specific\n","    to the footpath dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"footpath\"\n","\n","    # Train on 1 GPU and 2 images per GPU.\n","    # Batch size is 2 (Batch size = GPU_COUNT * IMAGES_PER_GPU). \n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 2 # use large RAM mode 12GB for two images\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 3  # background + 1 class\n","\n","    STEPS_PER_EPOCH = 300\n","\n","    #VALIDATION_STEPS = 10\n","\t\n","\t  # Skip detections with < 90% confidence\n","    DETECTION_MIN_CONFIDENCE = 0.9\n","\n","    IMAGE_CHANNEL_COUNT = 3\n","\n","    # Image mean (RGB)\n","    DEPTH_MEAN_PIXEL = np.array([89.6, 149.9, 51.9])\n","    COLOR_MEAN_PIXEL = np.array([123.7, 116.8, 103.9])\n","    \n","config = FootpathConfig()\n","config.display()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7g-9ETcQ27y","colab_type":"text"},"source":["## Dataset\n"]},{"cell_type":"code","metadata":{"id":"AVhpnFdqQ5c7","colab_type":"code","colab":{}},"source":["\"\"\"Color image frames dataset\"\"\"\n","# Dataset \n","class RGB_FootpathDataset(utils.Dataset):\n","    def load_footpath(self, dataset_dir, subset):\n","        \n","        # Add classes. \n","        self.add_class(\"footpath\", 1, \"footpath\")\n","        self.add_class(\"footpath\", 2, \"highway\")\n","        self.add_class(\"footpath\", 3, \"obstacle\")\n","\n","        name_dict = {}\n","        for i in range(1, len(self.class_info)):\n","          name_dict[self.class_info[i][\"name\"]] = self.class_info[i][\"id\"]\n","        name_dict_keys = []\n","        for key, value in name_dict.items():\n","          name_dict_keys.append(key)\n","\n","        # Train or validation dataset?\n","        assert subset in [\"train\", \"val\", \"test\"]\n","        files_dir = os.path.join(dataset_dir, subset)\n","\n","        # Note: In VIA 2.0, regions was changed from a dict to a list.\n","        # VGG Image Annotator (up to version 2.0.10) saves each image in the form:\n","        # { 'filename': 'footpath_13.png',\n","        #   'size': 463211, \n","        #   'regions': [\n","        #          {\n","        #           'shape_attributes': {\n","        #               'name': 'polygon'\n","        #               'all_points_x': [...],\n","        #               'all_points_y': [...]},\n","        #           'region_attributes': {'footpath': 'footpath'}}\n","        #           ... more regions ...\n","        #          ],\n","        #   'file_attributes': {}\n","        # }\n","        annotations = json.load(open(os.path.join(files_dir, \"via_region_data_\"+subset+\".json\")))\n","        annotations = list(annotations.values())  # don't need the dict keys\n","        annotations = [a for a in annotations if a['regions']]\n","\n","        # Select the annotations by classes \n","        temp_a = []\n","        bool_flag = False\n","        for a in annotations:\n","          names = [r['region_attributes']['footpath'] for r in a['regions']]\n","          for key, value in name_dict.items():\n","            bool_flag = bool_flag or key in names\n","          if bool_flag:\n","            temp_a.append(a)\n","            bool_flag = False\n","        annotations = temp_a\n","        temp_a = []\n","\n","        # Add images\n","        name_ids = []\n","        full_name_ids = []\n","        for a in annotations:\n","            names = [r['region_attributes']['footpath'] for r in a['regions']]\n","            for name in names:\n","              if name in name_dict_keys:\n","                name_ids.append(name_dict[name])\n","                full_name_ids.append(name_dict[name])\n","              else:\n","                full_name_ids.append(-1)\n","            # image_name = a['filename'].replace('color.png', 'depth_visualization.png')\n","            image_name = a['filename']\n","            image_path = os.path.join(files_dir, image_name)\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","            self.add_image(\"footpath\", image_id=image_name, path=image_path, width=width, height=height,\n","                           class_ids=name_ids, full_class_ids=full_name_ids)\n","            name_ids = []\n","            full_name_ids = []\n","\n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for an image.\n","       Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","\n","        # If not a footpath dataset image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"footpath\":\n","            return super(self.__class__, self).load_mask(image_id)\n","        full_name_ids = image_info['full_class_ids']\n","\n","        info = self.image_info[image_id]\n","        mask_file_id = info[\"id\"].replace('color.png', 'mask.npy')\n","        depth_visualization_image_path = info[\"path\"]\n","        parent_dir = os.path.dirname(depth_visualization_image_path)\n","        mask_file_path = os.path.join(parent_dir, mask_file_id)\n","\n","        original_mask = np.load(mask_file_path)\n","        original_class_ids = np.array(full_name_ids, dtype=np.int32)\n","\n","        id_list = []\n","        for item in self.class_info:\n","          id_list.append(item['id'])\n","\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(full_name_ids)])\n","        class_ids_list = []\n","        count = 0\n","        for i in range(0, len(full_name_ids)):\n","          if full_name_ids[i] in id_list:\n","            class_ids_list.append(full_name_ids[i])\n","            mask[:, :, count] = original_mask[:, :, i]\n","            count = count + 1\n","        class_ids = np.array(class_ids_list, dtype=np.int32)\n","        mask = mask[:, :, 0:count]\n","\n","        # Return mask, and array of class IDs of each instance.\n","        return mask, class_ids\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"footpath\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)\n","\n","############################################################################################\n","\"\"\"Depth image frames dataset\"\"\"\n","# Dataset \n","class Depth_FootpathDataset(utils.Dataset):\n","\n","    def load_footpath(self, dataset_dir, subset):\n","        from tqdm import tqdm\n","        # Add classes. \n","        self.add_class(\"footpath\", 1, \"footpath\")\n","        self.add_class(\"footpath\", 2, \"highway\")\n","        self.add_class(\"footpath\", 3, \"obstacle\")\n","\n","        name_dict = {}\n","        for i in range(1, len(self.class_info)):\n","          name_dict[self.class_info[i][\"name\"]] = self.class_info[i][\"id\"]\n","        name_dict_keys = []\n","        for key, value in name_dict.items():\n","          name_dict_keys.append(key)\n","\n","        # Train or validation dataset?\n","        assert subset in [\"train\", \"val\", \"test\"]\n","        files_dir = os.path.join(dataset_dir, subset)\n","\n","        # Note: In VIA 2.0, regions was changed from a dict to a list.\n","        # VGG Image Annotator (up to version 2.0.10) saves each image in the form:\n","        # { 'filename': 'footpath_13.png',\n","        #   'size': 463211, \n","        #   'regions': [\n","        #          {\n","        #           'shape_attributes': {\n","        #               'name': 'polygon'\n","        #               'all_points_x': [...],\n","        #               'all_points_y': [...]},\n","        #           'region_attributes': {'footpath': 'footpath'}}\n","        #           ... more regions ...\n","        #          ],\n","        #   'file_attributes': {}\n","        # }\n","        annotations = json.load(open(os.path.join(files_dir, \"via_region_data_\"+subset+\".json\")))\n","        annotations = list(annotations.values())  # don't need the dict keys\n","        annotations = [a for a in annotations if a['regions']]\n","\n","        # Select the annotations by classes \n","        temp_a = []\n","        bool_flag = False\n","        for a in annotations:\n","          names = [r['region_attributes']['footpath'] for r in a['regions']]\n","          for key, value in name_dict.items():\n","            bool_flag = bool_flag or key in names\n","          if bool_flag:\n","            temp_a.append(a)\n","            bool_flag = False\n","        annotations = temp_a\n","        temp_a = []\n","\n","        # Add images\n","        name_ids = []\n","        full_name_ids = []\n","        for a in annotations:\n","            names = [r['region_attributes']['footpath'] for r in a['regions']]\n","            for name in names:\n","              if name in name_dict_keys:\n","                name_ids.append(name_dict[name])\n","                full_name_ids.append(name_dict[name])\n","              else:\n","                full_name_ids.append(-1)\n","            image_name = a['filename'].replace('color.png', 'depth_visualization.png')\n","            image_path = os.path.join(files_dir, image_name)\n","            image = skimage.io.imread(image_path)\n","            height, width = image.shape[:2]\n","            self.add_image(\"footpath\", image_id=image_name, path=image_path, width=width, height=height,\n","                           class_ids=name_ids, full_class_ids=full_name_ids)\n","            name_ids = []\n","            full_name_ids = []\n","        \n","\n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for an image.\n","       Returns:\n","        masks: A bool array of shape [height, width, instance count] with\n","            one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","\n","        # If not a footpath dataset image, delegate to parent class.\n","        image_info = self.image_info[image_id]\n","        if image_info[\"source\"] != \"footpath\":\n","            return super(self.__class__, self).load_mask(image_id)\n","        full_name_ids = image_info['full_class_ids']\n","\n","        info = self.image_info[image_id]\n","        mask_file_id = info[\"id\"].replace('depth_visualization.png', 'mask.npy')\n","        depth_visualization_image_path = info[\"path\"]\n","        parent_dir = os.path.dirname(depth_visualization_image_path)\n","        mask_file_path = os.path.join(parent_dir, mask_file_id)\n","\n","        original_mask = np.load(mask_file_path)\n","        original_class_ids = np.array(full_name_ids, dtype=np.int32)\n","\n","        id_list = []\n","        for item in self.class_info:\n","          id_list.append(item['id'])\n","\n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(full_name_ids)])\n","        class_ids_list = []\n","        count = 0\n","        for i in range(0, len(full_name_ids)):\n","          if full_name_ids[i] in id_list:\n","            class_ids_list.append(full_name_ids[i])\n","            mask[:, :, count] = original_mask[:, :, i]\n","            count = count + 1\n","        class_ids = np.array(class_ids_list, dtype=np.int32)\n","        mask = mask[:, :, 0:count]\n","\n","        # Return mask, and array of class IDs of each instance.\n","        return mask, class_ids\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"footpath\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRxby2T9UCnP","colab_type":"code","colab":{}},"source":["\"\"\"Initialize color image frames dataset\"\"\"\n","# Training dataset\n","rgb_dataset_train = RGB_FootpathDataset()\n","rgb_dataset_train.load_footpath(dataset_dir, \"train\")\n","rgb_dataset_train.prepare()\n","\n","# Validation dataset\n","rgb_dataset_val = RGB_FootpathDataset()\n","rgb_dataset_val.load_footpath(dataset_dir, \"val\")\n","rgb_dataset_val.prepare()\n","\n","# Test dataset\n","rgb_dataset_test = RGB_FootpathDataset()\n","rgb_dataset_test.load_footpath(dataset_dir, \"test\")\n","rgb_dataset_test.prepare()\n","\n","######################################################\n","\"\"\"Initialize depth image frames dataset\"\"\"\n","# Training dataset\n","depth_dataset_train = Depth_FootpathDataset()\n","depth_dataset_train.load_footpath(dataset_dir, \"train\")\n","depth_dataset_train.prepare()\n","\n","# Validation dataset\n","depth_dataset_val = Depth_FootpathDataset()\n","depth_dataset_val.load_footpath(dataset_dir, \"val\")\n","depth_dataset_val.prepare()\n","\n","# Test dataset\n","depth_dataset_test = Depth_FootpathDataset()\n","depth_dataset_test.load_footpath(dataset_dir, \"test\")\n","depth_dataset_test.prepare()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z3NdpatwVcBM","colab_type":"text"},"source":["## Create Model"]},{"cell_type":"code","metadata":{"id":"eXfcYXoqVd8n","colab_type":"code","colab":{}},"source":["# Create Model\n","# Create model in training mode\n","model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfZHhFCrVjtp","colab_type":"code","colab":{}},"source":["# Which weights to start with?\n","init_with = \"coco\"  # imagenet, coco, or last\n","\n","if init_with == \"imagenet\":\n","    model.load_weights(model.get_imagenet_weights(), by_name=True)\n","elif init_with == \"coco\":\n","    # Load weights trained on MS COCO, but skip layers that\n","    # are different due to the different number of classes\n","    # See README for instructions to download the COCO weights\n","    model.load_weights(COCO_MODEL_PATH, by_name=True,\n","                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n","                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n","elif init_with == \"diode\":\n","    model.load_weights(DIODE_DEPTH_MODEL_PATH, by_name=True, exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \"mrcnn_bbox\", \"mrcnn_mask\"])\n","    \n","elif init_with == \"last\":\n","    # Load the last model you trained and continue training\n","    model.load_weights(model.find_last(), by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJuv_6OLVsjU","colab_type":"text"},"source":["## Training\n","\n","Train in two stages:\n","1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n","\n","2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."]},{"cell_type":"code","metadata":{"id":"EQD_C8AuVveO","colab_type":"code","colab":{}},"source":["# Data augmentation configuration\n","aug = iaa.Sometimes(9/10, iaa.SomeOf((1, 3), [iaa.Affine(translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)}), \n","                                              iaa.Affine(scale=(0.5, 1.5)), \n","                                              iaa.Affine(rotate=(-10, 10)), \n","                                              iaa.Fliplr(1)]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"df6gqGy1Vzlx","colab_type":"code","colab":{}},"source":["# Training\n","# Train the head branches\n","# Passing layers=\"heads\" freezes all layers except the head\n","# layers. You can also pass a regular expression to select\n","# which layers to train by name pattern.\n","print(\"Training all the branches\")\n","model.train(dataset_train, dataset_val, \n","            learning_rate=config.LEARNING_RATE, \n","            epochs=50, \n","            layers='heads', \n","            augmentation=aug)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fQ1BDSNXV2ra","colab_type":"code","colab":{}},"source":["# Fine tune all layers\n","# Passing layers=\"all\" trains all layers. You can also \n","# pass a regular expression to select which layers to\n","# train by name pattern.\n","print(\"Fine-tuning all layers\")\n","model.train(dataset_train, dataset_val, \n","            learning_rate=config.LEARNING_RATE / 10,\n","            epochs=10, \n","            layers=\"all\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgA9eznjV7iu","colab_type":"code","colab":{}},"source":["# Save weights\n","# Typically not needed because callbacks save after every epoch\n","# Uncomment to save manually\n","# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_diode_depth_pretrain.h5\")\n","# model.keras_model.save_weights(model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDC7eNbVaf84","colab_type":"code","colab":{}},"source":["# Tensorboard\n","%load_ext tensorboard\n","%tensorboard --logdir '/content/drive/My Drive/logs/footpath20200814T2038'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ucmhz3z_WBj1","colab_type":"text"},"source":["## Detection"]},{"cell_type":"code","metadata":{"id":"m-4BEH11WA7q","colab_type":"code","colab":{}},"source":["# Detection\n","class InferenceConfig(FootpathConfig):\n","    GPU_COUNT = 1\n","    IMAGES_PER_GPU = 1\n","\n","inference_config = InferenceConfig()\n","\n","# Recreate the model in inference mode\n","model = modellib.MaskRCNN(mode=\"inference\", config=inference_config, model_dir=MODEL_DIR)\n","\n","# Get path to saved weights\n","# Either set a specific path or find last trained weights\n","# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n","model_path = model.find_last()\n","\n","# Load trained weights\n","print(\"Loading weights from \", model_path)\n","model.load_weights(model_path, by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsS5q_E6aCZ9","colab_type":"code","colab":{}},"source":["# Test on a random image\n","image_id = random.choice(dataset_test.image_ids)\n","#image_id = 10\n","original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","    modellib.load_image_gt(dataset_test, inference_config, image_id, use_mini_mask=False)\n","\n","log(\"original_image\", original_image)\n","log(\"image_meta\", image_meta)\n","log(\"gt_class_id\", gt_class_id)\n","log(\"gt_bbox\", gt_bbox)\n","log(\"gt_mask\", gt_mask)\n","\n","# Display ground truth\n","visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, dataset_train.class_names)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uU6FOresaH52","colab_type":"code","colab":{}},"source":["# Prediction(detection) results\n","results = model.detect([original_image], verbose=1)\n","\n","r = results[0]\n","visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], dataset_test.class_names, r['scores'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w7NhW_ERaS8I","colab_type":"text"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"V4FBU3SgaOIx","colab_type":"code","colab":{}},"source":["#Evaluation\n","# Compute VOC-Style mAP @ IoU=0.5\n","# Running on 10 images. Increase for better accuracy.\n","image_ids = np.random.choice(dataset_test.image_ids, 20)\n","APs = []\n","for image_id in image_ids:\n","    # Load image and ground truth data\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n","        modellib.load_image_gt(dataset_test, inference_config,\n","                               image_id, use_mini_mask=False)\n","    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n","    # Run object detection\n","    results = model.detect([image], verbose=0)\n","    r = results[0]\n","    # Compute AP\n","    AP, precisions, recalls, overlaps =\\\n","        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n","                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n","    APs.append(AP)\n","    \n","print(\"mAP: \", np.mean(APs))"],"execution_count":null,"outputs":[]}]}